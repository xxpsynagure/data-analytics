{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg+Gd+5h4PLdkfn+GFJ5D/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxpsynagure/data-analytics/blob/main/Pragnya_4SO19CS110_BDA_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BDA Lab Assignment 3\n",
        "\n",
        "## 1. MapReduce program in Java to find occurrences of each word in a text file.\n",
        "\n",
        "### Step 1: Set up the environment and Start the Hadoop Server"
      ],
      "metadata": {
        "id": "V2ooxecPUwdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "EUT5BAvohsmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "4jARkwlTVW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r hadoop-3.3.4/ /usr/local/"
      ],
      "metadata": {
        "id": "tHBeUVMOVcJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbuSyUakUEU2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH4ORuPBU2sy",
        "outputId": "180944b3-ed14-4d69-df1e-7ae70584a30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Write Mapper and Reducer Program in Java and create a jar file\n"
      ],
      "metadata": {
        "id": "Fw4g_njr_qbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/mapper.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMkWRsNpeXhA",
        "outputId": "5b891b5b-a0e6-4690-e965-8b59042f89e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.wordcount.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import java.util.StringTokenizer;\r\n",
            "import org.apache.hadoop.io.IntWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Mapper;\r\n",
            "import org.apache.hadoop.io.LongWritable;\r\n",
            "\r\n",
            "public class WordCountMapper extends Mapper <LongWritable, Text, Text, IntWritable>\r\n",
            "{\r\n",
            "    private Text wordToken = new Text();\r\n",
            "    public void map(LongWritable key, Text value, Context context) throws IOException,\r\n",
            "    InterruptedException\r\n",
            "    {\r\n",
            "        StringTokenizer tokens = new StringTokenizer(value.toString()); //Dividing String into tokens\r\n",
            "        while (tokens.hasMoreTokens())\r\n",
            "        {\r\n",
            "            wordToken.set(tokens.nextToken());\r\n",
            "            context.write(wordToken, new IntWritable(1));\r\n",
            "        }\r\n",
            "    }\r\n",
            "}\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/reducer.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4MhAyJlec6i",
        "outputId": "b09a3903-5558-452c-a482-b59d14d92070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.wordcount.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import org.apache.hadoop.io.IntWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Reducer;\r\n",
            "public class WordCountReducer extends Reducer <Text, IntWritable, Text, IntWritable>\r\n",
            "{\r\n",
            "    private IntWritable count = new IntWritable();\r\n",
            "    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException\r\n",
            "    {\r\n",
            "// gurukul [1 1 1 1 1 1....]\r\n",
            "    int valueSum = 0;\r\n",
            "    for (IntWritable val : values)\r\n",
            "    {\r\n",
            "        valueSum += val.get();\r\n",
            "    }\r\n",
            "    count.set(valueSum);\r\n",
            "    context.write(key, count);\r\n",
            "    }\r\n",
            "}\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/driver.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOn6NUP_eg-U",
        "outputId": "925a657c-a289-4fff-b94c-7e4b058ba743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.wordcount.wc;\r\n",
            "import org.apache.hadoop.conf.Configuration;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.IntWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Job;\r\n",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
            "import org.apache.hadoop.util.GenericOptionsParser;\r\n",
            "\r\n",
            "public class WordCount\r\n",
            "{\r\n",
            "    public static void main(String[] args) throws Exception\r\n",
            "    {\r\n",
            "        Configuration conf = new Configuration();\r\n",
            "        String[] pathArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n",
            "        if (pathArgs.length < 2)\r\n",
            "        {\r\n",
            "            System.err.println(\"MR Project Usage: wordcount <input-path> [...] <output-path>\");\r\n",
            "            System.exit(2);\r\n",
            "        }\r\n",
            "        Job wcJob = Job.getInstance(conf, \"MapReduce WordCount\");\r\n",
            "        wcJob.setJarByClass(WordCount.class);\r\n",
            "        wcJob.setMapperClass(WordCountMapper.class);\r\n",
            "        wcJob.setCombinerClass(WordCountReducer.class);\r\n",
            "        wcJob.setReducerClass(WordCountReducer.class);\r\n",
            "        wcJob.setOutputKeyClass(Text.class);\r\n",
            "        wcJob.setOutputValueClass(IntWritable.class);\r\n",
            "        for (int i = 0; i < pathArgs.length - 1; ++i)\r\n",
            "        {\r\n",
            "            FileInputFormat.addInputPath(wcJob, new Path(pathArgs[i]));\r\n",
            "        }\r\n",
            "        FileOutputFormat.setOutputPath(wcJob, new Path(pathArgs[pathArgs.length - 1]));\r\n",
            "        System.exit(wcJob.waitForCompletion(true) ? 0 : 1);\r\n",
            "    }\r\n",
            "}\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create an input text file for counting"
      ],
      "metadata": {
        "id": "erGdQIaBiRk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/input"
      ],
      "metadata": {
        "id": "QKr__aAu7E6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/input/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WJvKne6eIam",
        "outputId": "c08e9e16-8001-4156-e896-f25d1b77e1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good Morning\r\n",
            "Good Afternoon\r\n",
            "Good Evening\r\n",
            "Good Night"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Run the command to perform Map Reduce using WordCount.jar file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GtCRxFAwBAjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /content/WordCount.jar /content/input/input.txt /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgv3u9wNW0w8",
        "outputId": "792fefe0-783b-4ab4-a3ac-110c80d8cb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-11 18:48:41,027 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-11 18:48:41,161 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-11 18:48:41,161 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-11 18:48:41,318 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2022-10-11 18:48:41,348 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-11 18:48:41,900 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1831973220_0001\n",
            "2022-10-11 18:48:41,900 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-11 18:48:42,111 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-11 18:48:42,112 INFO mapreduce.Job: Running job: job_local1831973220_0001\n",
            "2022-10-11 18:48:42,121 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-11 18:48:42,131 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 18:48:42,131 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 18:48:42,132 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2022-10-11 18:48:42,197 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-11 18:48:42,199 INFO mapred.LocalJobRunner: Starting task: attempt_local1831973220_0001_m_000000_0\n",
            "2022-10-11 18:48:42,234 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 18:48:42,234 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 18:48:42,273 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 18:48:42,282 INFO mapred.MapTask: Processing split: file:/content/input/input.txt:0+54\n",
            "2022-10-11 18:48:42,449 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-11 18:48:42,449 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-11 18:48:42,459 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-11 18:48:42,459 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-11 18:48:42,459 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-11 18:48:42,481 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-10-11 18:48:42,500 INFO mapred.LocalJobRunner: \n",
            "2022-10-11 18:48:42,501 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-11 18:48:42,501 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-11 18:48:42,501 INFO mapred.MapTask: bufstart = 0; bufend = 84; bufvoid = 104857600\n",
            "2022-10-11 18:48:42,501 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600\n",
            "2022-10-11 18:48:42,518 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-11 18:48:42,542 INFO mapred.Task: Task:attempt_local1831973220_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 18:48:42,556 INFO mapred.LocalJobRunner: map\n",
            "2022-10-11 18:48:42,556 INFO mapred.Task: Task 'attempt_local1831973220_0001_m_000000_0' done.\n",
            "2022-10-11 18:48:42,573 INFO mapred.Task: Final Counters for attempt_local1831973220_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=5840\n",
            "\t\tFILE: Number of bytes written=641078\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4\n",
            "\t\tMap output records=8\n",
            "\t\tMap output bytes=84\n",
            "\t\tMap output materialized bytes=73\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=8\n",
            "\t\tCombine output records=5\n",
            "\t\tSpilled Records=5\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=338690048\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=54\n",
            "2022-10-11 18:48:42,573 INFO mapred.LocalJobRunner: Finishing task: attempt_local1831973220_0001_m_000000_0\n",
            "2022-10-11 18:48:42,574 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-11 18:48:42,579 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-11 18:48:42,581 INFO mapred.LocalJobRunner: Starting task: attempt_local1831973220_0001_r_000000_0\n",
            "2022-10-11 18:48:42,592 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 18:48:42,592 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 18:48:42,593 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 18:48:42,596 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@df782c\n",
            "2022-10-11 18:48:42,598 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-11 18:48:42,626 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-11 18:48:42,635 INFO reduce.EventFetcher: attempt_local1831973220_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-11 18:48:42,681 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1831973220_0001_m_000000_0 decomp: 69 len: 73 to MEMORY\n",
            "2022-10-11 18:48:42,685 INFO reduce.InMemoryMapOutput: Read 69 bytes from map-output for attempt_local1831973220_0001_m_000000_0\n",
            "2022-10-11 18:48:42,687 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 69, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->69\n",
            "2022-10-11 18:48:42,690 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-11 18:48:42,691 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 18:48:42,692 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-11 18:48:42,701 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 18:48:42,702 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
            "2022-10-11 18:48:42,703 INFO reduce.MergeManagerImpl: Merged 1 segments, 69 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-11 18:48:42,706 INFO reduce.MergeManagerImpl: Merging 1 files, 73 bytes from disk\n",
            "2022-10-11 18:48:42,707 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-11 18:48:42,707 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 18:48:42,710 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
            "2022-10-11 18:48:42,711 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 18:48:42,718 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-10-11 18:48:42,722 INFO mapred.Task: Task:attempt_local1831973220_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 18:48:42,726 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 18:48:42,726 INFO mapred.Task: Task attempt_local1831973220_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-11 18:48:42,732 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1831973220_0001_r_000000_0' to file:/content/output\n",
            "2022-10-11 18:48:42,733 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2022-10-11 18:48:42,734 INFO mapred.Task: Task 'attempt_local1831973220_0001_r_000000_0' done.\n",
            "2022-10-11 18:48:42,735 INFO mapred.Task: Final Counters for attempt_local1831973220_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6018\n",
            "\t\tFILE: Number of bytes written=641210\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=73\n",
            "\t\tReduce input records=5\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=5\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=338690048\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=59\n",
            "2022-10-11 18:48:42,735 INFO mapred.LocalJobRunner: Finishing task: attempt_local1831973220_0001_r_000000_0\n",
            "2022-10-11 18:48:42,735 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-11 18:48:43,118 INFO mapreduce.Job: Job job_local1831973220_0001 running in uber mode : false\n",
            "2022-10-11 18:48:43,119 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-11 18:48:43,120 INFO mapreduce.Job: Job job_local1831973220_0001 completed successfully\n",
            "2022-10-11 18:48:43,133 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=11858\n",
            "\t\tFILE: Number of bytes written=1282288\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4\n",
            "\t\tMap output records=8\n",
            "\t\tMap output bytes=84\n",
            "\t\tMap output materialized bytes=73\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=8\n",
            "\t\tCombine output records=5\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=73\n",
            "\t\tReduce input records=5\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=10\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=677380096\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=54\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "TTX-SOeZBjoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/output/part-r-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3HO7wXEYPv0",
        "outputId": "51098f3e-cbdf-4774-aebd-0d5dad32f56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Afternoon\t1\n",
            "Evening\t1\n",
            "Good\t4\n",
            "Morning\t1\n",
            "Night\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  MapReduce program in Java to find sum of products sold in each country.\n",
        "\n",
        "\n",
        "### Step 1: Download the dataset needed\n",
        "\n",
        "### Step 2: Write Map Reduce Programs in Java to create the jar file\n"
      ],
      "metadata": {
        "id": "JEN7lN9oBvUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/mapper1.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRV0O5ff6q5",
        "outputId": "7e3810c3-9d9a-47b7-e242-a95f19fe3181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.productsum.wc;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.*;\r\n",
            "import org.apache.hadoop.mapred.*;\r\n",
            "public class SalesCountryDriver {\r\n",
            "public static void main(String[] args) {\r\n",
            "    // Create a configuration object for the job\r\n",
            "    JobClient my_client = new JobClient();\r\n",
            "    JobConf job_conf = new JobConf(SalesCountryDriver.class);\r\n",
            "    // Set a name of the Job\r\n",
            "    job_conf.setJobName(\"SalePerCountry\");\r\n",
            "    // Specify data type of output key and value\r\n",
            "    job_conf.setOutputKeyClass(Text.class);\r\n",
            "    job_conf.setOutputValueClass(IntWritable.class);\r\n",
            "    // Specify names of Mapper and Reducer Class\r\n",
            "    job_conf.setMapperClass(SalesMapper.class);\r\n",
            "    job_conf.setReducerClass(SalesCountryReducer.class);\r\n",
            "    // Specify formats of the data type of Input and output\r\n",
            "    job_conf.setInputFormat(TextInputFormat.class);\r\n",
            "    job_conf.setOutputFormat(TextOutputFormat.class);\r\n",
            "    FileInputFormat.setInputPaths(job_conf, new Path(args[0]));\r\n",
            "    FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));\r\n",
            "    my_client.setConf(job_conf);\r\n",
            "    try {\r\n",
            "        JobClient.runJob(job_conf);\r\n",
            "    } catch (Exception e) {\r\n",
            "        e.printStackTrace();\r\n",
            "    }\r\n",
            "}\r\n",
            "}\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/reducer1.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKdFEqIggAZ4",
        "outputId": "840dada7-8792-429f-bb38-f1b094a43df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.productsum.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import java.util.*;\r\n",
            "import org.apache.hadoop.io.IntWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapred.*;\r\n",
            "public class SalesCountryReducer extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\r\n",
            "    public void reduce(Text t_key, Iterator<IntWritable> values,\r\n",
            "    OutputCollector<Text, IntWritable> output, Reporter reporter)\r\n",
            "    throws IOException {\r\n",
            "        Text key = t_key;\r\n",
            "        int frequencyForCountry = 0;\r\n",
            "        while (values.hasNext()) {\r\n",
            "            IntWritable value = (IntWritable) values.next();\r\n",
            "            frequencyForCountry += value.get();\r\n",
            "        }\r\n",
            "        output.collect(key, new IntWritable(frequencyForCountry));\r\n",
            "    }\r\n",
            "}\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/driver1.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYh6eYEAgHyM",
        "outputId": "654c9bfa-eabc-46e5-a45b-a2306e85bcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.productsum.wc;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.*;\r\n",
            "import org.apache.hadoop.mapred.*;\r\n",
            "public class SalesCountryDriver {\r\n",
            "    public static void main(String[] args) {\r\n",
            "        JobClient my_client = new JobClient();\r\n",
            "        // Create a configuration object for the job\r\n",
            "        JobConf job_conf = new JobConf(SalesCountryDriver.class);\r\n",
            "        // Set a name of the Job\r\n",
            "        job_conf.setJobName(\"SalePerCountry\");\r\n",
            "        // Specify data type of output key and value\r\n",
            "        job_conf.setOutputKeyClass(Text.class);\r\n",
            "        job_conf.setOutputValueClass(IntWritable.class);\r\n",
            "        // Specify names of Mapper and Reducer Class\r\n",
            "        job_conf.setMapperClass(SalesMapper.class);\r\n",
            "        job_conf.setReducerClass(SalesCountryReducer.class);\r\n",
            "        // Specify formats of the data type of Input and output\r\n",
            "        job_conf.setInputFormat(TextInputFormat.class);\r\n",
            "        job_conf.setOutputFormat(TextOutputFormat.class);\r\n",
            "        FileInputFormat.setInputPaths(job_conf, new Path(args[0]));\r\n",
            "        FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));\r\n",
            "        my_client.setConf(job_conf);\r\n",
            "        try {\r\n",
            "            JobClient.runJob(job_conf);\r\n",
            "        } catch (Exception e) {\r\n",
            "            e.printStackTrace();\r\n",
            "        }\r\n",
            "    }\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Execution"
      ],
      "metadata": {
        "id": "r1MBmNG2bHGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /content/ProductSum.jar /content/input/SalesJan2009.csv /content/output1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cxLdObE6qP",
        "outputId": "1878074d-7e97-4667-adf2-b971deceabb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-11 19:34:32,028 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-11 19:34:32,183 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-11 19:34:32,183 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-11 19:34:32,206 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-11 19:34:32,285 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
            "2022-10-11 19:34:32,358 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-10-11 19:34:32,381 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-11 19:34:32,631 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local182147993_0001\n",
            "2022-10-11 19:34:32,631 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-11 19:34:32,830 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-11 19:34:32,833 INFO mapreduce.Job: Running job: job_local182147993_0001\n",
            "2022-10-11 19:34:32,853 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-11 19:34:32,859 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-10-11 19:34:32,867 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:34:32,867 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:34:32,919 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-11 19:34:32,924 INFO mapred.LocalJobRunner: Starting task: attempt_local182147993_0001_m_000000_0\n",
            "2022-10-11 19:34:32,964 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:34:32,966 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:34:33,003 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 19:34:33,011 INFO mapred.MapTask: Processing split: file:/content/input/SalesJan2009.csv:0+123637\n",
            "2022-10-11 19:34:33,026 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-10-11 19:34:33,140 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-11 19:34:33,140 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-11 19:34:33,140 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-11 19:34:33,140 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-11 19:34:33,140 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-11 19:34:33,150 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-10-11 19:34:33,208 INFO mapred.LocalJobRunner: \n",
            "2022-10-11 19:34:33,208 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-11 19:34:33,208 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-11 19:34:33,208 INFO mapred.MapTask: bufstart = 0; bufend = 15743; bufvoid = 104857600\n",
            "2022-10-11 19:34:33,208 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210404(104841616); length = 3993/6553600\n",
            "2022-10-11 19:34:33,235 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-11 19:34:33,253 INFO mapred.Task: Task:attempt_local182147993_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 19:34:33,255 INFO mapred.LocalJobRunner: file:/content/input/SalesJan2009.csv:0+123637\n",
            "2022-10-11 19:34:33,255 INFO mapred.Task: Task 'attempt_local182147993_0001_m_000000_0' done.\n",
            "2022-10-11 19:34:33,268 INFO mapred.Task: Final Counters for attempt_local182147993_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=128588\n",
            "\t\tFILE: Number of bytes written=654835\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=999\n",
            "\t\tMap output records=999\n",
            "\t\tMap output bytes=15743\n",
            "\t\tMap output materialized bytes=17747\n",
            "\t\tInput split bytes=88\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=999\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=359661568\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=123637\n",
            "2022-10-11 19:34:33,268 INFO mapred.LocalJobRunner: Finishing task: attempt_local182147993_0001_m_000000_0\n",
            "2022-10-11 19:34:33,270 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-11 19:34:33,274 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-11 19:34:33,278 INFO mapred.LocalJobRunner: Starting task: attempt_local182147993_0001_r_000000_0\n",
            "2022-10-11 19:34:33,291 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:34:33,291 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:34:33,291 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 19:34:33,296 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34f95b15\n",
            "2022-10-11 19:34:33,298 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-11 19:34:33,321 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-11 19:34:33,327 INFO reduce.EventFetcher: attempt_local182147993_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-11 19:34:33,371 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local182147993_0001_m_000000_0 decomp: 17743 len: 17747 to MEMORY\n",
            "2022-10-11 19:34:33,374 INFO reduce.InMemoryMapOutput: Read 17743 bytes from map-output for attempt_local182147993_0001_m_000000_0\n",
            "2022-10-11 19:34:33,377 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 17743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->17743\n",
            "2022-10-11 19:34:33,381 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-11 19:34:33,383 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:34:33,383 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-11 19:34:33,391 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 19:34:33,392 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 17731 bytes\n",
            "2022-10-11 19:34:33,403 INFO reduce.MergeManagerImpl: Merged 1 segments, 17743 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-11 19:34:33,404 INFO reduce.MergeManagerImpl: Merging 1 files, 17747 bytes from disk\n",
            "2022-10-11 19:34:33,405 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-11 19:34:33,405 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 19:34:33,408 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 17731 bytes\n",
            "2022-10-11 19:34:33,409 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:34:33,445 INFO mapred.Task: Task:attempt_local182147993_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 19:34:33,447 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:34:33,447 INFO mapred.Task: Task attempt_local182147993_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-11 19:34:33,449 INFO output.FileOutputCommitter: Saved output of task 'attempt_local182147993_0001_r_000000_0' to file:/content/output1\n",
            "2022-10-11 19:34:33,450 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2022-10-11 19:34:33,450 INFO mapred.Task: Task 'attempt_local182147993_0001_r_000000_0' done.\n",
            "2022-10-11 19:34:33,451 INFO mapred.Task: Final Counters for attempt_local182147993_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=164114\n",
            "\t\tFILE: Number of bytes written=673259\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=58\n",
            "\t\tReduce shuffle bytes=17747\n",
            "\t\tReduce input records=999\n",
            "\t\tReduce output records=58\n",
            "\t\tSpilled Records=999\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=359661568\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=677\n",
            "2022-10-11 19:34:33,451 INFO mapred.LocalJobRunner: Finishing task: attempt_local182147993_0001_r_000000_0\n",
            "2022-10-11 19:34:33,451 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-11 19:34:33,840 INFO mapreduce.Job: Job job_local182147993_0001 running in uber mode : false\n",
            "2022-10-11 19:34:33,841 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-11 19:34:33,843 INFO mapreduce.Job: Job job_local182147993_0001 completed successfully\n",
            "2022-10-11 19:34:33,852 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=292702\n",
            "\t\tFILE: Number of bytes written=1328094\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=999\n",
            "\t\tMap output records=999\n",
            "\t\tMap output bytes=15743\n",
            "\t\tMap output materialized bytes=17747\n",
            "\t\tInput split bytes=88\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=58\n",
            "\t\tReduce shuffle bytes=17747\n",
            "\t\tReduce input records=999\n",
            "\t\tReduce output records=58\n",
            "\t\tSpilled Records=1998\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=719323136\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=123637\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/output1/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4J8YKU-Zqlb",
        "outputId": "908bc406-80ac-4e84-e0fa-3eedc2734571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argentina\t1\n",
            "Australia\t38\n",
            "Austria\t7\n",
            "Bahrain\t1\n",
            "Belgium\t8\n",
            "Bermuda\t1\n",
            "Brazil\t5\n",
            "Bulgaria\t1\n",
            "CO\t1\n",
            "Canada\t76\n",
            "Cayman Isls\t1\n",
            "China\t1\n",
            "Costa Rica\t1\n",
            "Country\t1\n",
            "Czech Republic\t3\n",
            "Denmark\t15\n",
            "Dominican Republic\t1\n",
            "Finland\t2\n",
            "France\t27\n",
            "Germany\t25\n",
            "Greece\t1\n",
            "Guatemala\t1\n",
            "Hong Kong\t1\n",
            "Hungary\t3\n",
            "Iceland\t1\n",
            "India\t2\n",
            "Ireland\t49\n",
            "Israel\t1\n",
            "Italy\t15\n",
            "Japan\t2\n",
            "Jersey\t1\n",
            "Kuwait\t1\n",
            "Latvia\t1\n",
            "Luxembourg\t1\n",
            "Malaysia\t1\n",
            "Malta\t2\n",
            "Mauritius\t1\n",
            "Moldova\t1\n",
            "Monaco\t2\n",
            "Netherlands\t22\n",
            "New Zealand\t6\n",
            "Norway\t16\n",
            "Philippines\t2\n",
            "Poland\t2\n",
            "Romania\t1\n",
            "Russia\t1\n",
            "South Africa\t5\n",
            "South Korea\t1\n",
            "Spain\t12\n",
            "Sweden\t13\n",
            "Switzerland\t36\n",
            "Thailand\t2\n",
            "The Bahamas\t2\n",
            "Turkey\t6\n",
            "Ukraine\t1\n",
            "United Arab Emirates\t6\n",
            "United Kingdom\t100\n",
            "United States\t462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MapReduce program in Java to find average sales ( AVG(price) ) per country.\n",
        "\n",
        "\n",
        "### Step 1: Download the dataset needed\n",
        "\n",
        "### Step 2: Write Map Reduce Programs in Java to create the jar file"
      ],
      "metadata": {
        "id": "9DmpW2FVGK5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/mapper2.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yrekTtBgnJD",
        "outputId": "23c564fb-ec6e-45e9-93ea-66f0cc7d4bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.avgsales.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import org.apache.hadoop.conf.Configuration;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.FloatWritable;\r\n",
            "import org.apache.hadoop.io.LongWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Job;\r\n",
            "import org.apache.hadoop.mapreduce.Mapper;\r\n",
            "import org.apache.hadoop.mapreduce.Reducer;\r\n",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
            "public class Average_sales {\r\n",
            "    public static class MapperClass extends Mapper<LongWritable, Text, Text, FloatWritable> {\r\n",
            "        public void map(LongWritable key, Text salesrecord, Context con)\r\n",
            "        throws IOException, InterruptedException {\r\n",
            "            String[] word = salesrecord.toString().split(\",\");\r\n",
            "            String country = word[7];\r\n",
            "            try {\r\n",
            "                Float price = Float.parseFloat(word[2]);\r\n",
            "                con.write(new Text(country), new FloatWritable(price));\r\n",
            "            } catch (Exception e) {\r\n",
            "                e.printStackTrace();\r\n",
            "            }\r\n",
            "        }\r\n",
            "    }\r\n",
            "}\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/reducer2.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DbA2WRbgrno",
        "outputId": "544637ac-bb49-4112-e959-7c475dbfbad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.average.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import org.apache.hadoop.conf.Configuration;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.FloatWritable;\r\n",
            "import org.apache.hadoop.io.LongWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Job;\r\n",
            "import org.apache.hadoop.mapreduce.Mapper;\r\n",
            "import org.apache.hadoop.mapreduce.Reducer;\r\n",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
            "public class Average_reducer{\r\n",
            "    public static class ReducerClass extends\r\n",
            "    Reducer<Text, FloatWritable, Text, Text> {\r\n",
            "        public void reduce(Text key, Iterable<FloatWritable> valueList,\r\n",
            "        Context con) throws IOException, InterruptedException {\r\n",
            "            try {\r\n",
            "                Float total = (float) 0;\r\n",
            "                int count = 0;\r\n",
            "                for (FloatWritable var : valueList) {\r\n",
            "                    total += var.get();\r\n",
            "                    System.out.println(\"reducer \" + var.get());\r\n",
            "                    count++;\r\n",
            "                }\r\n",
            "                Float avg = (Float) total / count;\r\n",
            "                // String out = \"Total: \" + total + \" :: \" + \"Average: \" + avg;\r\n",
            "                String out = \"Average: \" + avg;\r\n",
            "                con.write(key, new Text(out));\r\n",
            "            } catch (Exception e) {\r\n",
            "                e.printStackTrace();\r\n",
            "            }\r\n",
            "        }\r\n",
            "    }\r\n",
            "}\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/Programs/driver2.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oowWtP-mgy27",
        "outputId": "7c6f3a94-29a1-4101-dcb6-9c4c86baceed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package com.avgsales.wc;\r\n",
            "import java.io.IOException;\r\n",
            "import org.apache.hadoop.conf.Configuration;\r\n",
            "import org.apache.hadoop.fs.Path;\r\n",
            "import org.apache.hadoop.io.FloatWritable;\r\n",
            "import org.apache.hadoop.io.LongWritable;\r\n",
            "import org.apache.hadoop.io.Text;\r\n",
            "import org.apache.hadoop.mapreduce.Job;\r\n",
            "import org.apache.hadoop.mapreduce.Mapper;\r\n",
            "import org.apache.hadoop.mapreduce.Reducer;\r\n",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
            "public class Averagemain {\r\n",
            "    public static void main(String[] args) {\r\n",
            "        Configuration conf = new Configuration();\r\n",
            "        try {\r\n",
            "            Job job = Job.getInstance(conf, \"FindAverageOfPrice\");\r\n",
            "            job.setJarByClass(Average_sales.class);\r\n",
            "            job.setMapperClass(MapperClass.class);\r\n",
            "            job.setReducerClass(ReducerClass.class);\r\n",
            "            job.setOutputKeyClass(Text.class);\r\n",
            "            job.setOutputValueClass(FloatWritable.class);\r\n",
            "            Path p1 = new Path(args[0]);\r\n",
            "            Path p2 = new Path(args[1]);\r\n",
            "            FileInputFormat.addInputPath(job, p1);\r\n",
            "            FileOutputFormat.setOutputPath(job, p2);\r\n",
            "            System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n",
            "        } catch (IOException e) {\r\n",
            "            e.printStackTrace();\r\n",
            "        } catch (ClassNotFoundException e) {\r\n",
            "            e.printStackTrace();\r\n",
            "        } catch (InterruptedException e) {\r\n",
            "            e.printStackTrace();\r\n",
            "        }\r\n",
            "    }\r\n",
            "}\r\n",
            " \r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Execution"
      ],
      "metadata": {
        "id": "52o-khbXbkyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /content/AvgSales.jar /content/input/SalesJan2009.csv /content/output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b1badc-ecb5-4643-922a-eac07ee49fac",
        "id": "HY_UyeWJbkyb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-11 19:27:59,229 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-11 19:27:59,412 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-11 19:27:59,412 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-11 19:27:59,539 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
            "2022-10-11 19:27:59,605 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2022-10-11 19:27:59,641 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-11 19:27:59,917 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local23432650_0001\n",
            "2022-10-11 19:27:59,918 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-11 19:28:00,126 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-11 19:28:00,127 INFO mapreduce.Job: Running job: job_local23432650_0001\n",
            "2022-10-11 19:28:00,135 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-11 19:28:00,142 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:28:00,142 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:28:00,143 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2022-10-11 19:28:00,200 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-11 19:28:00,201 INFO mapred.LocalJobRunner: Starting task: attempt_local23432650_0001_m_000000_0\n",
            "2022-10-11 19:28:00,242 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:28:00,244 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:28:00,286 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 19:28:00,293 INFO mapred.MapTask: Processing split: file:/content/input/SalesJan2009.csv:0+123637\n",
            "2022-10-11 19:28:00,431 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-11 19:28:00,431 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-11 19:28:00,431 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-11 19:28:00,431 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-11 19:28:00,431 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-11 19:28:00,436 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "java.lang.NumberFormatException: For input string: \"Price\"\n",
            "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
            "\tat java.base/jdk.internal.math.FloatingDecimal.parseFloat(FloatingDecimal.java:122)\n",
            "\tat java.base/java.lang.Float.parseFloat(Float.java:455)\n",
            "\tat com.average.wc.Average_sales$MapperClass.map(Average_sales.java:32)\n",
            "\tat com.average.wc.Average_sales$MapperClass.map(Average_sales.java:1)\n",
            "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n",
            "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:800)\n",
            "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:348)\n",
            "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "java.lang.NumberFormatException: For input string: \"\"13\"\n",
            "\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n",
            "\tat java.base/jdk.internal.math.FloatingDecimal.parseFloat(FloatingDecimal.java:122)\n",
            "\tat java.base/java.lang.Float.parseFloat(Float.java:455)\n",
            "\tat com.average.wc.Average_sales$MapperClass.map(Average_sales.java:32)\n",
            "\tat com.average.wc.Average_sales$MapperClass.map(Average_sales.java:1)\n",
            "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n",
            "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:800)\n",
            "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:348)\n",
            "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "2022-10-11 19:28:00,537 INFO mapred.LocalJobRunner: \n",
            "2022-10-11 19:28:00,540 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-11 19:28:00,540 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-11 19:28:00,540 INFO mapred.MapTask: bufstart = 0; bufend = 15724; bufvoid = 104857600\n",
            "2022-10-11 19:28:00,540 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210412(104841648); length = 3985/6553600\n",
            "2022-10-11 19:28:00,563 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-11 19:28:00,580 INFO mapred.Task: Task:attempt_local23432650_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 19:28:00,583 INFO mapred.LocalJobRunner: map\n",
            "2022-10-11 19:28:00,584 INFO mapred.Task: Task 'attempt_local23432650_0001_m_000000_0' done.\n",
            "2022-10-11 19:28:00,593 INFO mapred.Task: Final Counters for attempt_local23432650_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=130231\n",
            "\t\tFILE: Number of bytes written=652591\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=999\n",
            "\t\tMap output records=997\n",
            "\t\tMap output bytes=15724\n",
            "\t\tMap output materialized bytes=17724\n",
            "\t\tInput split bytes=101\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=997\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=384827392\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=123637\n",
            "2022-10-11 19:28:00,593 INFO mapred.LocalJobRunner: Finishing task: attempt_local23432650_0001_m_000000_0\n",
            "2022-10-11 19:28:00,594 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-11 19:28:00,604 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-11 19:28:00,604 INFO mapred.LocalJobRunner: Starting task: attempt_local23432650_0001_r_000000_0\n",
            "2022-10-11 19:28:00,626 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-11 19:28:00,626 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-11 19:28:00,627 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-11 19:28:00,631 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6428ac4c\n",
            "2022-10-11 19:28:00,634 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-11 19:28:00,659 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-11 19:28:00,662 INFO reduce.EventFetcher: attempt_local23432650_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-11 19:28:00,728 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local23432650_0001_m_000000_0 decomp: 17720 len: 17724 to MEMORY\n",
            "2022-10-11 19:28:00,733 INFO reduce.InMemoryMapOutput: Read 17720 bytes from map-output for attempt_local23432650_0001_m_000000_0\n",
            "2022-10-11 19:28:00,736 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 17720, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->17720\n",
            "2022-10-11 19:28:00,739 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-11 19:28:00,740 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:28:00,740 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-11 19:28:00,747 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 19:28:00,747 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 17708 bytes\n",
            "2022-10-11 19:28:00,754 INFO reduce.MergeManagerImpl: Merged 1 segments, 17720 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-11 19:28:00,754 INFO reduce.MergeManagerImpl: Merging 1 files, 17724 bytes from disk\n",
            "2022-10-11 19:28:00,756 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-11 19:28:00,756 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-11 19:28:00,757 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 17708 bytes\n",
            "2022-10-11 19:28:00,757 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:28:00,762 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 7500.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1800.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 250.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1250.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 800.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 7500.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 2100.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 3600.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "reducer 1200.0\n",
            "2022-10-11 19:28:00,850 INFO mapred.Task: Task:attempt_local23432650_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-11 19:28:00,851 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-11 19:28:00,851 INFO mapred.Task: Task attempt_local23432650_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-11 19:28:00,853 INFO output.FileOutputCommitter: Saved output of task 'attempt_local23432650_0001_r_000000_0' to file:/content/output2\n",
            "2022-10-11 19:28:00,854 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2022-10-11 19:28:00,854 INFO mapred.Task: Task 'attempt_local23432650_0001_r_000000_0' done.\n",
            "2022-10-11 19:28:00,855 INFO mapred.Task: Final Counters for attempt_local23432650_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=165711\n",
            "\t\tFILE: Number of bytes written=671775\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=56\n",
            "\t\tReduce shuffle bytes=17724\n",
            "\t\tReduce input records=997\n",
            "\t\tReduce output records=56\n",
            "\t\tSpilled Records=997\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=384827392\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1460\n",
            "2022-10-11 19:28:00,856 INFO mapred.LocalJobRunner: Finishing task: attempt_local23432650_0001_r_000000_0\n",
            "2022-10-11 19:28:00,856 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-11 19:28:01,132 INFO mapreduce.Job: Job job_local23432650_0001 running in uber mode : false\n",
            "2022-10-11 19:28:01,133 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-11 19:28:01,134 INFO mapreduce.Job: Job job_local23432650_0001 completed successfully\n",
            "2022-10-11 19:28:01,141 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=295942\n",
            "\t\tFILE: Number of bytes written=1324366\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=999\n",
            "\t\tMap output records=997\n",
            "\t\tMap output bytes=15724\n",
            "\t\tMap output materialized bytes=17724\n",
            "\t\tInput split bytes=101\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=56\n",
            "\t\tReduce shuffle bytes=17724\n",
            "\t\tReduce input records=997\n",
            "\t\tReduce output records=56\n",
            "\t\tSpilled Records=1994\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=769654784\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=123637\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "Wqd381ofbSAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/output2/part-r-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913d426e-2f6e-434d-807b-652976ff94ea",
        "id": "D4sgumV3bSAq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argentina\tAverage: 1200.0\n",
            "Australia\tAverage: 1705.2632\n",
            "Austria\tAverage: 1542.8572\n",
            "Bahrain\tAverage: 1200.0\n",
            "Belgium\tAverage: 1500.0\n",
            "Bermuda\tAverage: 1200.0\n",
            "Brazil\tAverage: 2460.0\n",
            "Bulgaria\tAverage: 1200.0\n",
            "Canada\tAverage: 1642.1052\n",
            "Cayman Isls\tAverage: 1200.0\n",
            "China\tAverage: 1200.0\n",
            "Costa Rica\tAverage: 1200.0\n",
            "Czech Republic\tAverage: 2000.0\n",
            "Denmark\tAverage: 1200.0\n",
            "Dominican Republic\tAverage: 1200.0\n",
            "Finland\tAverage: 1200.0\n",
            "France\tAverage: 1966.6666\n",
            "Germany\tAverage: 1680.0\n",
            "Greece\tAverage: 1200.0\n",
            "Guatemala\tAverage: 1200.0\n",
            "Hong Kong\tAverage: 1200.0\n",
            "Hungary\tAverage: 1200.0\n",
            "Iceland\tAverage: 1200.0\n",
            "India\tAverage: 1200.0\n",
            "Ireland\tAverage: 1426.5306\n",
            "Israel\tAverage: 1200.0\n",
            "Italy\tAverage: 2520.0\n",
            "Japan\tAverage: 1200.0\n",
            "Jersey\tAverage: 1200.0\n",
            "Kuwait\tAverage: 1200.0\n",
            "Latvia\tAverage: 1200.0\n",
            "Luxembourg\tAverage: 1200.0\n",
            "Malaysia\tAverage: 1200.0\n",
            "Malta\tAverage: 2400.0\n",
            "Mauritius\tAverage: 3600.0\n",
            "Moldova\tAverage: 1200.0\n",
            "Monaco\tAverage: 1200.0\n",
            "Netherlands\tAverage: 2031.8182\n",
            "New Zealand\tAverage: 1200.0\n",
            "Norway\tAverage: 1350.0\n",
            "Philippines\tAverage: 1200.0\n",
            "Poland\tAverage: 1200.0\n",
            "Romania\tAverage: 1200.0\n",
            "Russia\tAverage: 3600.0\n",
            "South Africa\tAverage: 2460.0\n",
            "South Korea\tAverage: 1200.0\n",
            "Spain\tAverage: 1400.0\n",
            "Sweden\tAverage: 1753.8462\n",
            "Switzerland\tAverage: 2133.3333\n",
            "Thailand\tAverage: 2400.0\n",
            "The Bahamas\tAverage: 1200.0\n",
            "Turkey\tAverage: 1200.0\n",
            "Ukraine\tAverage: 1200.0\n",
            "United Arab Emirates\tAverage: 2000.0\n",
            "United Kingdom\tAverage: 1440.0\n",
            "United States\tAverage: 1595.238\n"
          ]
        }
      ]
    }
  ]
}